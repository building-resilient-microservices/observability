server.port=8081
spring.application.name=kafka-consumer-demo
spring.kafka.bootstrap-servers=localhost:9092
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.IntegerDeserializer
spring.kafka.consumer.value-deserializer=org.springframework.kafka.support.serializer.JsonDeserializer
spring.kafka.consumer.properties.spring.json.trusted.packages=*
management.endpoints.web.exposure.include=prometheus, metrics, health, info, loggers, flyway
# All traces will be sent to latency analysis tool, for production use < 1.0 (100%) [default=0.1]
management.tracing.enabled=true
management.tracing.sampling.probability=1.0
# For Exemplars to work we need histogram buckets
management.metrics.distribution.percentiles-histogram.http.server.requests=true
management.metrics.distribution.percentiles-histogram.method.observed=true
# The step size to use in computing windowed statistics like max. The default is 1 minute.
# To get the most out of these statistics, align the step interval to be close to your scrape interval.
management.prometheus.metrics.export.step=5s
# Commons tags are applied to all meters
management.observations.key-values.some.tag=some.value
# IMPORTANT! for apps running version lower than Spring 3.2
# traceID and spanId are predefined MDC keys - we want the logs to include them as part of log level
# logging.pattern.level=%5p [${spring.application.name:},%X{traceId:-},%X{spanId:-}]
#exmaple of log level configuration by Class
logging.level.com.example.messaging.controller.FactController=INFO
logging.level.net.ttddyy.dsproxy.listener=DEBUG
#logging.level.io.micrometer=TRACE
#logging.level.io.opentelemetry=TRACE
# Database configuration
spring.datasource.url=jdbc:mysql://localhost:3307/fact-db
spring.datasource.username=root
spring.datasource.password=password
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
spring.flyway.locations=classpath:db/migration
